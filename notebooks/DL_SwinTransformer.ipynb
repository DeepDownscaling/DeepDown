{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "721a1923-df03-4ace-a939-8a62df36c1a8",
   "metadata": {},
   "source": [
    "# Precipitation and temperature downscaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a4c7b-f424-4168-87db-fb4c421e81f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import sys\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append('/storage/homefs/no21h426/DL-downscaling/')\n",
    "import os\n",
    "import yaml\n",
    "import math\n",
    "import warnings\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask\n",
    "from time import time\n",
    "\n",
    "# Import torch\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader, sampler, TensorDataset\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "# To make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# Config matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=10)\n",
    "mpl.rc('xtick', labelsize=8)\n",
    "mpl.rc('ytick', labelsize=8)\n",
    "\n",
    "# Utils\n",
    "from utils.data_loader import *\n",
    "from utils.utils_plot import *\n",
    "from utils.utils_loss import *\n",
    "from utils.helpers import *\n",
    "from utils.Datagenerators import *\n",
    "from models.SRGAN import *\n",
    "\n",
    "# Try dask.distributed and see if the performance improves...\n",
    "from dask.distributed import Client\n",
    "c = Client(n_workers=os.cpu_count()-2, threads_per_worker=1)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\"divide by zero encountered in divide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec53edd-fea4-4a06-86cf-9759b3e91883",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cuda Avaliable :\", torch.cuda.is_available())\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b6a454-bf5b-408c-949a-fe32115d6524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths and constant\n",
    "with open('../config.yaml') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "# Display options\n",
    "PLOT_DATA_FULL_EXTENT = False\n",
    "PLOT_DATA_CROPPED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc85a8e0-7dd6-4a02-b461-873f5de721b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DEM = config['PATH_DEM']\n",
    "PATH_ERA5_025 = config['PATH_ERA5_025']  # Original ERA5 0.25°\n",
    "PATH_ERA5_100 = config['PATH_ERA5_100']  # ERA5 1°\n",
    "PATH_MCH = config['PATH_MeteoSwiss']  \n",
    "    \n",
    "NUM_CHANNELS_IN = config['NUM_CHANNELS_IN']\n",
    "NUM_CHANNELS_OUT = config['NUM_CHANNELS_OUT']\n",
    "lr = config['lr']\n",
    "\n",
    "# Data options\n",
    "DATE_START = config['DATE_START']\n",
    "DATE_END = config['DATE_END']\n",
    "YY_TRAIN = config['YY_TRAIN']  \n",
    "YY_TEST = config['YY_TEST']  \n",
    "LEVELS = config['LEVELS'] \n",
    "RESOL_LOW = config['RESOL_LOW'] \n",
    "INPUT_VARIABLES = config['INPUT_VARIABLES']\n",
    "INPUT_PATHS = [PATH_ERA5_025 + '/precipitation', PATH_ERA5_025 + '/temperature']\n",
    "DUMP_DATA_TO_PICKLE = config['DUMP_DATA_TO_PICKLE']\n",
    "\n",
    "\n",
    "NUM_CHANNELS_IN = config['NUM_CHANNELS_IN']\n",
    "NUM_CHANNELS_OUT = config['NUM_CHANNELS_OUT']\n",
    "lr = config['lr']\n",
    "# Crop on a smaller region\n",
    "DO_CROP = config['DO_CROP']\n",
    "# I reduce the area of crop now, to avoid NA\n",
    "CROP_X = [2680000, 2760000]  # with NAN: [2720000, 2770000]\n",
    "CROP_Y = [1180000, 1260000]  # with NAN: [1290000, 1320000]\n",
    "\n",
    "device = config['device']\n",
    "dtype = config['dtype'] \n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "h, w = config['lowres_shape']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd263aa1-8933-46db-bd91-c47d88c5b17b",
   "metadata": {},
   "source": [
    "## Target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f11a81f-dcc9-4819-b3b0-ad33ea8cf928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load target data\n",
    "target = load_target_data(DATE_START, DATE_END, PATH_MCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab299048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the axes of the final target domain based on temperature \n",
    "x_axis = target.TabsD.x\n",
    "y_axis = target.TabsD.y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e696c2",
   "metadata": {},
   "source": [
    "## Input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adba65ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = load_input_data(DATE_START, DATE_END, PATH_DEM, INPUT_VARIABLES, INPUT_PATHS, \n",
    "                             LEVELS, RESOL_LOW, x_axis, y_axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68469053-d1f8-4733-ab88-ce5922bcf55d",
   "metadata": {},
   "source": [
    "## Crop domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678ff05a-9d5d-436b-bca2-36d9f8d2be00",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_CROP:\n",
    "    input_data = input_data.sel(x=slice(min(CROP_X), max(CROP_X)), y=slice(max(CROP_Y), min(CROP_Y)))\n",
    "    target = target.sel(x=slice(min(CROP_X), max(CROP_X)), y=slice(max(CROP_Y), min(CROP_Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e275c8-038a-4012-bb9a-3783af9a0d7d",
   "metadata": {},
   "source": [
    "## Split sample and data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9046d4-1db6-44ba-947b-e82198696de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "x_train = input_data.sel(time=slice('1999', '2011')) \n",
    "x_valid = input_data.sel(time=slice('2012', '2015')) \n",
    "x_test = input_data.sel(time=slice('2016', '2021'))\n",
    "\n",
    "y_train = target.sel(time=slice('1999', '2011'))\n",
    "y_valid = target.sel(time=slice('2012', '2015'))\n",
    "y_test = target.sel(time=slice('2006', '2011'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe836e2-e95d-4540-a7b2-198459a1b51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the variables to use as input and output\n",
    "input_vars = {'topo' : None, 'tp': None, 't': LEVELS}\n",
    "output_vars = ['RhiresD', 'TabsD'] #['RhiresD', 'TabsD', 'TmaxD', 'TminD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27464664-a9c9-431a-97ec-ff51a8961df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = DataGenerator(x_train, y_train, input_vars, output_vars)\n",
    "loader_train = torch.utils.data.DataLoader(training_set, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd3bcd8-b6f7-4d1d-9fca-42d7ae7b4cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "valid_set = DataGenerator(x_valid, y_valid, input_vars, output_vars, shuffle=False, mean=training_set.mean, std=training_set.std)\n",
    "loader_val = torch.utils.data.DataLoader(valid_set, batch_size=32)\n",
    "\n",
    "# Test\n",
    "test_set = DataGenerator(x_test, y_test, input_vars, output_vars, shuffle=False, mean=training_set.mean, std=training_set.std)\n",
    "loader_test = torch.utils.data.DataLoader(test_set, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccdb8e7-0119-4d6f-a7c9-6266c9db0674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to make sure the range on the input and output images is correct, and they're the correct shape\n",
    "testx, testy = valid_set.__getitem__(3)\n",
    "print(\"x shape: \", testx.shape)\n",
    "print(\"y shape: \", testy.shape)\n",
    "print(\"x min: \", torch.min(testx))\n",
    "print(\"x max: \", torch.max(testx))\n",
    "print(\"y min: \", torch.min(testy))\n",
    "print(\"y max: \",torch.max(testy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e1edb3-b3d4-476d-ace0-7202f71014aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637a8ccc-b703-4ae8-af39-afd413646501",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(loader_train))\n",
    "x, y = data\n",
    "print('Shape of x:', x.shape)\n",
    "print('Shape of y:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da98a192-f9e0-4a8b-82e2-5080770c9fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test the Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfed46fd-617c-479e-9611-acfa89b6f7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float32 \n",
    "input_size=[80,80] #config['lowres_shape']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd69e4c-f581-4897-bff4-673a3d7c27b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5410a1de-24e3-47ea-b76a-58deb9f9aab4",
   "metadata": {},
   "source": [
    "### Swin Transformer\n",
    "It aims to address the limitations of traditional Vision Transformers by introducing a hierarchical architecture and incorporating shifted windows for local context modeling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc3a5ab-fdb7-4cb7-aef6-188f89634804",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e25acc-2350-4f8c-be3c-b3dde847468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Test ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66612aac-a196-4a99-825e-714be0f2c955",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.swin_transformer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e89797c-5aa8-4b6b-9f87-0c6f8360cb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f4c79f-38ff-42a2-9703-e6e4e1d7c614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from einops import rearrange\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, H, W, C)\n",
    "        window_size (int): window size\n",
    "\n",
    "    Returns:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "        window_size (int): Window size\n",
    "        H (int): Height of image\n",
    "        W (int): Width of image\n",
    "\n",
    "    Returns:\n",
    "        x: (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x\n",
    "\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        # define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input features with shape of (num_windows*B, N, C)\n",
    "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
    "        \"\"\"\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n",
    "\n",
    "    def flops(self, N):\n",
    "        # calculate flops for 1 window with token length of N\n",
    "        flops = 0\n",
    "        # qkv = self.qkv(x)\n",
    "        flops += N * self.dim * 3 * self.dim\n",
    "        # attn = (q @ k.transpose(-2, -1))\n",
    "        flops += self.num_heads * N * (self.dim // self.num_heads) * N\n",
    "        #  x = (attn @ v)\n",
    "        flops += self.num_heads * N * N * (self.dim // self.num_heads)\n",
    "        # x = self.proj(x)\n",
    "        flops += N * self.dim * self.dim\n",
    "        return flops\n",
    "\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    r\"\"\" Swin Transformer Block.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resulotion.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Window size.\n",
    "        shift_size (int): Shift size for SW-MSA.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
    "        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        if min(self.input_resolution) <= self.window_size:\n",
    "            # if window size is larger than input resolution, we don't partition windows\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.input_resolution)\n",
    "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            # calculate attention mask for SW-MSA\n",
    "            H, W = self.input_resolution\n",
    "            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n",
    "            h_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            w_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            cnt = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    img_mask[:, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "\n",
    "            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
    "            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
    "            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "        else:\n",
    "            attn_mask = None\n",
    "\n",
    "        self.register_buffer(\"attn_mask\", attn_mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        \n",
    "        # assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "        \n",
    "        # cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        # partition windows\n",
    "        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # W-MSA/SW-MSA\n",
    "        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n",
    "\n",
    "        \n",
    "        \n",
    "        # reverse cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "        x = x.view(B, H * W, C)\n",
    "\n",
    "        # FFN\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \" \\\n",
    "               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        H, W = self.input_resolution\n",
    "        # norm1\n",
    "        flops += self.dim * H * W\n",
    "        # W-MSA/SW-MSA\n",
    "        nW = H * W / self.window_size / self.window_size\n",
    "        flops += nW * self.attn.flops(self.window_size * self.window_size)\n",
    "        # mlp\n",
    "        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio\n",
    "        # norm2\n",
    "        flops += self.dim * H * W\n",
    "        return flops    \n",
    "    \n",
    "class PatchMerging(nn.Module):\n",
    "    r\"\"\" Patch Merging Layer.\n",
    "\n",
    "    Args:\n",
    "        input_resolution (tuple[int]): Resolution of input feature.\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"input_resolution={self.input_resolution}, dim={self.dim}\"\n",
    "\n",
    "    def flops(self):\n",
    "        H, W = self.input_resolution\n",
    "        flops = H * W * self.dim\n",
    "        flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim\n",
    "        return flops\n",
    "\n",
    "\n",
    "# Dual up-sample\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, input_resolution, in_channels, scale_factor):\n",
    "        super(UpSample, self).__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.factor = scale_factor\n",
    "\n",
    "\n",
    "        if self.factor == 2:\n",
    "            self.conv = nn.Conv2d(in_channels, in_channels//2, 1, 1, 0, bias=False)\n",
    "            self.up_p = nn.Sequential(nn.Conv2d(in_channels, 2*in_channels, 1, 1, 0, bias=False),\n",
    "                                      nn.PReLU(),\n",
    "                                      nn.PixelShuffle(self.factor),\n",
    "                                      nn.Conv2d(in_channels//2, in_channels//2, 1, stride=1, padding=0, bias=False))\n",
    "\n",
    "            self.up_b = nn.Sequential(nn.Conv2d(in_channels, in_channels, 1, 1, 0),\n",
    "                                      nn.PReLU(),\n",
    "                                      nn.Upsample(scale_factor=self.factor, mode='bilinear', align_corners=False),\n",
    "                                      nn.Conv2d(in_channels, in_channels // 2, 1, stride=1, padding=0, bias=False))\n",
    "        elif self.factor == 4:\n",
    "            self.conv = nn.Conv2d(2*in_channels, in_channels, 1, 1, 0, bias=False)\n",
    "            self.up_p = nn.Sequential(nn.Conv2d(in_channels, 16 * in_channels, 1, 1, 0, bias=False),\n",
    "                                      nn.PReLU(),\n",
    "                                      nn.PixelShuffle(self.factor),\n",
    "                                      nn.Conv2d(in_channels, in_channels, 1, stride=1, padding=0, bias=False))\n",
    "\n",
    "            self.up_b = nn.Sequential(nn.Conv2d(in_channels, in_channels, 1, 1, 0),\n",
    "                                      nn.PReLU(),\n",
    "                                      nn.Upsample(scale_factor=self.factor, mode='bilinear', align_corners=False),\n",
    "                                      nn.Conv2d(in_channels, in_channels, 1, stride=1, padding=0, bias=False))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B, L = H*W, C\n",
    "        \"\"\"\n",
    "        if type(self.input_resolution) == int:\n",
    "            H = self.input_resolution\n",
    "            W = self.input_resolution\n",
    "\n",
    "        elif type(self.input_resolution) == tuple:\n",
    "            H, W = self.input_resolution\n",
    "\n",
    "        B, L, C = x.shape\n",
    "        x = x.view(B, H, W, C)  # B, H, W, C\n",
    "        x = x.permute(0, 3, 1, 2)  # B, C, H, W\n",
    "        x_p = self.up_p(x)  # pixel shuffle\n",
    "        x_b = self.up_b(x)  # bilinear\n",
    "        out = self.conv(torch.cat([x_p, x_b], dim=1))\n",
    "        out = out.permute(0, 2, 3, 1)  # B, H, W, C\n",
    "        if self.factor == 2:\n",
    "            out = out.view(B, -1, C // 2)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class BasicLayer(nn.Module):\n",
    "    \"\"\" A basic Swin Transformer layer for one stage.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resolution.\n",
    "        depth (int): Number of blocks.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Local window size.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.depth = depth\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        # build blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n",
    "                                 num_heads=num_heads, window_size=window_size,\n",
    "                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                                 mlp_ratio=mlp_ratio,\n",
    "                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                                 drop=drop, attn_drop=attn_drop,\n",
    "                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                                 norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "\n",
    "        # patch merging layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        for blk in self.blocks:\n",
    "            if self.use_checkpoint:\n",
    "                x = checkpoint.checkpoint(blk, x)\n",
    "            else:\n",
    "                x = blk(x)\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        for blk in self.blocks:\n",
    "            flops += blk.flops()\n",
    "        if self.downsample is not None:\n",
    "            flops += self.downsample.flops()\n",
    "        return flops\n",
    "\n",
    "\n",
    "class BasicLayer_up(nn.Module):\n",
    "    \"\"\" A basic Swin Transformer layer for one stage.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resolution.\n",
    "        depth (int): Number of blocks.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Local window size.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., norm_layer=nn.LayerNorm, upsample=None, use_checkpoint=False):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.depth = depth\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        # build blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n",
    "                                 num_heads=num_heads, window_size=window_size,\n",
    "                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                                 mlp_ratio=mlp_ratio,\n",
    "                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                                 drop=drop, attn_drop=attn_drop,\n",
    "                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                                 norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "\n",
    "        # patch merging layer\n",
    "        if upsample is not None:\n",
    "            self.upsample = UpSample(input_resolution, in_channels=dim, scale_factor=2)\n",
    "        else:\n",
    "            self.upsample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        for blk in self.blocks:\n",
    "            if self.use_checkpoint:\n",
    "                x = checkpoint.checkpoint(blk, x)\n",
    "            else:\n",
    "                x = blk(x)\n",
    "        if self.upsample is not None:\n",
    "            x = self.upsample(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    r\"\"\" Image to Patch Embedding\n",
    "\n",
    "    Args:\n",
    "        img_size (int): Image size.  Default: 224.\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = patches_resolution\n",
    "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        # FIXME look at relaxing size constraints\n",
    "        # assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "        #    f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "    def flops(self):\n",
    "        Ho, Wo = self.patches_resolution\n",
    "        flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[0] * self.patch_size[1])\n",
    "        if self.norm is not None:\n",
    "            flops += Ho * Wo * self.embed_dim\n",
    "        return flops\n",
    "\n",
    "\n",
    "class SUNet(nn.Module):\n",
    "    r\"\"\" Swin Transformer\n",
    "        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n",
    "          https://arxiv.org/pdf/2103.14030\n",
    "\n",
    "    Args:\n",
    "        img_size (int | tuple(int)): Input image size. Default 224\n",
    "        patch_size (int | tuple(int)): Patch size. Default: 4\n",
    "        in_chans (int): Number of input image channels. Default: 3\n",
    "\n",
    "        embed_dim (int): Patch embedding dimension. Default: 96\n",
    "        depths (tuple(int)): Depth of each Swin Transformer layer.\n",
    "        num_heads (tuple(int)): Number of attention heads in different layers.\n",
    "        window_size (int): Window size. Default: 7\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n",
    "        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n",
    "        drop_rate (float): Dropout rate. Default: 0\n",
    "        attn_drop_rate (float): Attention dropout rate. Default: 0\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n",
    "        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n",
    "        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n",
    "        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3, out_chans=3, out_size = 224, \n",
    "                 embed_dim=96, depths=[2, 2, 2, 2], num_heads=[3, 6, 12, 24],\n",
    "                 window_size=7, mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
    "                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,\n",
    "                 use_checkpoint=False, final_upsample=\"Dual up-sample\", **kwargs):\n",
    "        super(SUNet, self).__init__()\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.out_chans = out_chans\n",
    "        self.num_layers = len(depths)\n",
    "        self.out_size = out_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ape = ape\n",
    "        self.patch_norm = patch_norm\n",
    "        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n",
    "        self.num_features_up = int(embed_dim * 2)\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.final_upsample = final_upsample\n",
    "        self.prelu = nn.PReLU()\n",
    "        self.conv_first = nn.Conv2d(in_chans, embed_dim, 3, 1, 1)\n",
    "\n",
    "        # split image into non-overlapping patches\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=embed_dim, embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        patches_resolution = self.patch_embed.patches_resolution\n",
    "        self.patches_resolution = patches_resolution\n",
    "\n",
    "        # absolute position embedding\n",
    "        if self.ape:\n",
    "            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "            trunc_normal_(self.absolute_pos_embed, std=.02)\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "        # build encoder and bottleneck layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n",
    "                               input_resolution=(patches_resolution[0] // (2 ** i_layer),\n",
    "                                                 patches_resolution[1] // (2 ** i_layer)),\n",
    "                               depth=depths[i_layer],\n",
    "                               num_heads=num_heads[i_layer],\n",
    "                               window_size=window_size,\n",
    "                               mlp_ratio=self.mlp_ratio,\n",
    "                               qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                               drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
    "                               norm_layer=norm_layer,\n",
    "                               downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,\n",
    "                               use_checkpoint=use_checkpoint)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # build decoder layers\n",
    "        self.layers_up = nn.ModuleList()\n",
    "        self.concat_back_dim = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            concat_linear = nn.Linear(2 * int(embed_dim * 2 ** (self.num_layers - 1 - i_layer)),\n",
    "                                      int(embed_dim * 2 ** (\n",
    "                                              self.num_layers - 1 - i_layer))) if i_layer > 0 else nn.Identity()\n",
    "            if i_layer == 0:\n",
    "                layer_up = UpSample(input_resolution=patches_resolution[0] // (2 ** (self.num_layers - 1 - i_layer)),\n",
    "                                    in_channels=int(embed_dim * 2 ** (self.num_layers - 1 - i_layer)), scale_factor=2)\n",
    "            else:\n",
    "                layer_up = BasicLayer_up(dim=int(embed_dim * 2 ** (self.num_layers - 1 - i_layer)),\n",
    "                                         input_resolution=(\n",
    "                                             patches_resolution[0] // (2 ** (self.num_layers - 1 - i_layer)),\n",
    "                                             patches_resolution[1] // (2 ** (self.num_layers - 1 - i_layer))),\n",
    "                                         depth=depths[(self.num_layers - 1 - i_layer)],\n",
    "                                         num_heads=num_heads[(self.num_layers - 1 - i_layer)],\n",
    "                                         window_size=window_size,\n",
    "                                         mlp_ratio=self.mlp_ratio,\n",
    "                                         qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                                         drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                                         drop_path=dpr[sum(depths[:(self.num_layers - 1 - i_layer)]):sum(\n",
    "                                             depths[:(self.num_layers - 1 - i_layer) + 1])],\n",
    "                                         norm_layer=norm_layer,\n",
    "                                         upsample=UpSample if (i_layer < self.num_layers - 1) else None,\n",
    "                                         use_checkpoint=use_checkpoint)\n",
    "            self.layers_up.append(layer_up)\n",
    "            self.concat_back_dim.append(concat_linear)\n",
    "\n",
    "        self.norm = norm_layer(self.num_features)\n",
    "        self.norm_up = norm_layer(self.embed_dim)\n",
    "\n",
    "        if self.final_upsample == \"Dual up-sample\":\n",
    "            self.up = UpSample(input_resolution=(img_size // patch_size, img_size // patch_size),\n",
    "                               in_channels=embed_dim, scale_factor=4)\n",
    "            self.output = nn.Conv2d(in_channels=embed_dim, out_channels=self.out_chans, kernel_size=3, stride=1,\n",
    "                                    padding=1, bias=False)  # kernel = 1\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'absolute_pos_embed'}\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {'relative_position_bias_table'}\n",
    "\n",
    "    # Encoder and Bottleneck\n",
    "    def forward_features(self, x):\n",
    "        residual = x\n",
    "        x = self.patch_embed(x)\n",
    "        if self.ape:\n",
    "            x = x + self.absolute_pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        x_downsample = []\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x_downsample.append(x)\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.norm(x)  # B L C\n",
    "\n",
    "        return x, residual, x_downsample\n",
    "\n",
    "    # Dencoder and Skip connection\n",
    "    def forward_up_features(self, x, x_downsample):\n",
    "        for inx, layer_up in enumerate(self.layers_up):\n",
    "            if inx == 0:\n",
    "                x = layer_up(x)\n",
    "            else:\n",
    "                x = torch.cat([x, x_downsample[3 - inx]], -1)  # concat last dimension\n",
    "                x = self.concat_back_dim[inx](x)\n",
    "                x = layer_up(x)\n",
    "\n",
    "        x = self.norm_up(x)  # B L C\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "\n",
    "    def up_x4(self, x):\n",
    "        H, W = self.patches_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input features has wrong size\"\n",
    "\n",
    "        if self.final_upsample == \"Dual up-sample\":\n",
    "            x = self.up(x)\n",
    "            # x = x.view(B, 4 * H, 4 * W, -1)\n",
    "            x = x.permute(0, 3, 1, 2)  # B,C,H,W\n",
    "            \n",
    "    \n",
    "            x = F.interpolate(x, size=self.out_size, mode='bilinear', align_corners=False)\n",
    "    \n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_first(x)\n",
    "        x, residual, x_downsample = self.forward_features(x)\n",
    "        x = self.forward_up_features(x, x_downsample)\n",
    "        x = self.up_x4(x)\n",
    "        out = self.output(x)\n",
    "        # x = x + residual\n",
    "        return out\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        flops += self.patch_embed.flops()\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            flops += layer.flops()\n",
    "        flops += self.num_features * self.patches_resolution[0] * self.patches_resolution[1] // (2 ** self.num_layers)\n",
    "        flops += self.num_features * self.out_chans\n",
    "        return flops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1961aa5e-01df-4dc0-9a80-ee6186fe1390",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SUNet(img_size=80, patch_size=5, in_chans=4, out_chans=2, out_size =80,\n",
    "                  embed_dim=96, depths=[8, 8, 8, 8],\n",
    "                  num_heads=[8, 8, 8, 8],\n",
    "                  window_size=20, mlp_ratio=4., qkv_bias=True, qk_scale=2,\n",
    "                  drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
    "                  norm_layer=nn.LayerNorm, ape=False, patch_norm=True,\n",
    "                  use_checkpoint=False, final_upsample=\"Dual up-sample\")\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea19e7a0-ffc2-4bcf-a879-ea6988c8ac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=x.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee798302-2040-4e28-a720-c950c707a37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e4ba83-b8c8-4b75-9174-37d6bb50fd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = model(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23a15ea-5b47-4412-8d3b-4e08d57e430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40571db9-bcf2-41cd-9807-a153c6f19561",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "lr=0.0001\n",
    "#optimizer = optim.SGD(model_with_custom_head.parameters(), lr=0.0001)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157b1101-7f08-48d4-b43d-ac5309d30fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_epoch(x, y_pred, y):\n",
    "    figsize = (9,4)\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.subplot(1,3,1) \n",
    "    # x[0,0,..] correspond to topo\n",
    "    plt.imshow(x[0,1,:,:].cpu().detach().numpy())\n",
    "    plt.title(\"Input Precip\")\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(y_pred[0,0,:,:].cpu().detach().numpy())\n",
    "    plt.title(\"Output Precip\")\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(y[0,0,:,:].cpu().detach().numpy())\n",
    "    plt.title(\"True Precip\")\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(x[0,2,:,:].cpu().detach().numpy())\n",
    "    plt.title(\"Input Temp\")\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(y_pred[0,1,:,:].cpu().detach().numpy())\n",
    "    plt.title(\"Output Temp\")\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(y[0,1,:,:].cpu().detach().numpy())\n",
    "    plt.title(\"True Temp\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83045d28-59cf-4cca-aa34-c3328eae7ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_epochs = 20\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    train_predictions = []\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(loader_train):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_train_loss += loss.item()\n",
    "        \n",
    "        train_predictions.append(outputs.detach().cpu().numpy())\n",
    "\n",
    "    train_loss = running_train_loss / len(loader_train)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    val_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(loader_val):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_val_loss += loss.item()\n",
    "            val_predictions.append(outputs.detach().cpu().numpy())\n",
    "\n",
    "    val_loss = running_val_loss / len(loader_val)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # Plot loss\n",
    "    #plt.plot(range(1, epoch + 2), train_losses, 'b-', label='Training Loss')\n",
    "    #plt.plot(range(1, epoch + 2), val_losses, 'r-', label='Validation Loss')\n",
    "    #plt.xlabel('Epoch')\n",
    "    #plt.ylabel('Loss')\n",
    "    #plt.title('Training and Validation Loss')\n",
    "    #plt.legend()\n",
    "    #plt.show()\n",
    "\n",
    "    # Plot predictions\n",
    "    #plot_epoch(inputs, outputs, targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eda48b-082a-441e-9469-75e0df6b4363",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, epoch + 2), train_losses, 'b-', label='Training Loss')\n",
    "plt.plot(range(1, epoch + 2), val_losses, 'r-', label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4fc048-8641-43aa-88d6-1e6c023cc2cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyTT",
   "language": "python",
   "name": "pytt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
