{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70fcacbc-46d5-4ed6-9925-e27fb819332e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/homefs/no21h426/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "import argparse\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append('/storage/homefs/no21h426/DL-downscaling/')\n",
    "import os\n",
    "import yaml\n",
    "import math\n",
    "import warnings\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask\n",
    "from time import time\n",
    "\n",
    "# Import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset, DataLoader, sampler, TensorDataset\n",
    "from torch.utils.data import sampler\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# Utils\n",
    "from utils.data_loader import *\n",
    "from utils.utils_plot import *\n",
    "from utils.utils_loss import *\n",
    "from utils.helpers import *\n",
    "from utils.Datagenerators import *\n",
    "from src.constants import *\n",
    "# Try dask.distributed and see if the performance improves...\n",
    "from dask.distributed import Client\n",
    "c = Client(n_workers=os.cpu_count()-2, threads_per_worker=1)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\"divide by zero encountered in divide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dcb537d-c0ce-4ef8-b3fc-f5eb1de804c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda Avaliable : True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"Cuda Avaliable :\", torch.cuda.is_available())\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9fcf302-5da8-4318-93d2-001da6d2d271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target data loaded from pickle.\n",
      "Input data loaded from pickle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/homefs/no21h426/.local/lib/python3.10/site-packages/distributed/client.py:3109: UserWarning: Sending large graph of size 115.97 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/storage/homefs/no21h426/.local/lib/python3.10/site-packages/distributed/client.py:3109: UserWarning: Sending large graph of size 115.97 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into RAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/homefs/no21h426/.local/lib/python3.10/site-packages/distributed/client.py:3109: UserWarning: Sending large graph of size 115.97 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into RAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/homefs/no21h426/.local/lib/python3.10/site-packages/distributed/client.py:3109: UserWarning: Sending large graph of size 35.69 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into RAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/homefs/no21h426/.local/lib/python3.10/site-packages/distributed/client.py:3109: UserWarning: Sending large graph of size 53.54 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "target = load_target_data(DATE_START, DATE_END, PATH_MCH)\n",
    "x_axis = target.TabsD.x\n",
    "y_axis = target.TabsD.y\n",
    "    \n",
    "\n",
    "input_data = load_input_data(DATE_START, DATE_END, PATH_DEM, INPUT_VARIABLES, INPUT_PATHS, \n",
    "                                 LEVELS, RESOL_LOW, x_axis, y_axis)\n",
    "\n",
    "\n",
    "\n",
    "if DO_CROP:\n",
    "    input_data = input_data.sel(x=slice(min(CROP_X), max(CROP_X)), y=slice(max(CROP_Y), min(CROP_Y)))\n",
    "    target = target.sel(x=slice(min(CROP_X), max(CROP_X)), y=slice(max(CROP_Y), min(CROP_Y)))\n",
    "\n",
    "\n",
    "# Split the data (small data for testing purposes)\n",
    "x_train = input_data.sel(time=slice('1999', '2011')) \n",
    "x_valid = input_data.sel(time=slice('2012', '2015')) \n",
    "x_test = input_data.sel(time=slice('2016', '2021'))\n",
    "\n",
    "y_train = target.sel(time=slice('1999', '2011'))\n",
    "y_valid = target.sel(time=slice('2012', '2015'))\n",
    "y_test = target.sel(time=slice('2006', '2011'))\n",
    "\n",
    "# Select the variables to use as input and output\n",
    "input_vars = {'topo' : None, 'tp': None, 't': LEVELS}\n",
    "output_vars = ['RhiresD', 'TabsD'] #['RhiresD', 'TabsD', 'TmaxD', 'TminD']\n",
    "\n",
    "training_set = DataGenerator(x_train, y_train, input_vars, output_vars)\n",
    "loader_train = torch.utils.data.DataLoader(training_set, batch_size=32)\n",
    "\n",
    "# Validation\n",
    "valid_set = DataGenerator(x_valid, y_valid, input_vars, output_vars, shuffle=False, mean=training_set.mean, std=training_set.std)\n",
    "loader_val = torch.utils.data.DataLoader(valid_set, batch_size=32)\n",
    "\n",
    "# Test\n",
    "test_set = DataGenerator(x_test, y_test, input_vars, output_vars, shuffle=False, mean=training_set.mean, std=training_set.std)\n",
    "loader_test = torch.utils.data.DataLoader(test_set, batch_size=32)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fa5c94-4668-4ffc-a18b-159ac93acd8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyTT",
   "language": "python",
   "name": "pytt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
