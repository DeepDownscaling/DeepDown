{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "408a4c7b-f424-4168-87db-fb4c421e81f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import os\n",
    "import yaml\n",
    "import math\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask\n",
    "from time import time\n",
    "\n",
    "dask.config.set({'array.slicing.split_large_chunks': False})\n",
    "\n",
    "# Import torch\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# To make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# Config matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=10)\n",
    "mpl.rc('xtick', labelsize=8)\n",
    "mpl.rc('ytick', labelsize=8)\n",
    "\n",
    "# Utils\n",
    "from utils.data_process import *\n",
    "from utils.utils_plot import *\n",
    "from utils.utils_gans import *\n",
    "\n",
    "# Try dask.distributed and see if the performance improves --because I have HDF5 issues..\n",
    "from dask.distributed import Client\n",
    "c = Client(n_workers=os.cpu_count()-2, threads_per_worker=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec53edd-fea4-4a06-86cf-9759b3e91883",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cuda Avaliable :\", torch.cuda.is_available())\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b6a454-bf5b-408c-949a-fe32115d6524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths and constant\n",
    "with open('config.yaml') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "# Paths\n",
    "PATH_P_low = config['PATH_ERA5_P']  # Original ERA5 0.25\n",
    "PATH_T_low = config['PATH_ERA5_T']  # Original ERA5 0.25\n",
    "PATH_high = config['PATH_MeteoSwiss']  # Note that Meteoswiss has a different coordinate system, but it doesn't matter here, as we only care about tensors\n",
    "\n",
    "# Some constants\n",
    "G = 9.80665\n",
    "\n",
    "# Options\n",
    "DATE_START = '1999-01-01'  # '1979-01-01'\n",
    "DATE_END = '2021-12-31'\n",
    "YY_TRAIN = [1999, 2015]  # [1979, 2015]\n",
    "YY_TEST = [2016, 2021]\n",
    "\n",
    "LEVELS = [300, 500, 700, 850, 1000]  # Available with CORDEX-CMIP6\n",
    "\n",
    "# Alternatives to proceed with the NaN:\n",
    "# 1. Crop the tensor\n",
    "# 2. Create a mask (cons: information loss during the training)?\n",
    "# 3. Fill the Nan (not a good option)\n",
    "E_high_crop = [2700000, 2770000]\n",
    "N_high_crop = [1190000, 1280000]\n",
    "# to try with NAN\n",
    "#E_high_crop = [2720000, 2770000]\n",
    "#N_high_crop = [1290000, 1320000]\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd263aa1-8933-46db-bd91-c47d88c5b17b",
   "metadata": {},
   "source": [
    "## Target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52499cbd-98fb-4787-b44f-b28e79e8cbab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read high resolution data\n",
    "pr = get_nc_data(PATH_high + '/RhiresD_v2.0_swiss.lv95/*nc', DATE_START, DATE_END)\n",
    "t_abs = get_nc_data(PATH_high + '/TabsD_v2.0_swiss.lv95/*nc', DATE_START, DATE_END)\n",
    "t_max = get_nc_data(PATH_high + '/TmaxD_v2.0_swiss.lv95/*nc', DATE_START, DATE_END)\n",
    "t_min = get_nc_data(PATH_high + '/TminD_v2.0_swiss.lv95/*nc', DATE_START, DATE_END) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db4f7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the axes of the final domain\n",
    "E_axis = t_abs.E\n",
    "N_axis = t_abs.N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85a0efe-4957-4652-87b1-e6c9224e0676",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize=(18,4))\n",
    "plot_map(axs[0], pr.E, pr.N, np.flipud(pr.RhiresD.mean(dim='time').to_numpy().squeeze()), title=\"Daily precipitation\", cmap=mpl.cm.YlGnBu)\n",
    "plot_map(axs[1], t_abs.E, t_abs.N, np.flipud(t_abs.TabsD.mean(dim='time').to_numpy().squeeze()), title=\"Daily Temperature\", cmap=mpl.cm.RdBu_r)\n",
    "plot_map(axs[2], t_max.E, t_max.N, np.flipud(t_max.TmaxD.mean(dim='time').to_numpy().squeeze()), title=\"Daily Maximum temperature\", cmap=mpl.cm.RdBu_r)\n",
    "plot_map(axs[3], t_min.E, t_min.N, np.flipud(t_min.TminD.mean(dim='time').to_numpy().squeeze()), title=\"Daily Minimum temperature\", cmap=mpl.cm.RdBu_r)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68469053-d1f8-4733-ab88-ce5922bcf55d",
   "metadata": {},
   "source": [
    "### Cropped domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15167c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_d1 = pr.sel(E=slice(np.nanmin(E_high_crop), np.nanmax(E_high_crop)), N = slice(np.nanmin(N_high_crop), np.nanmax(N_high_crop)))\n",
    "t_abs_d1 = t_abs.sel(E=slice(np.nanmin(E_high_crop), np.nanmax(E_high_crop)), N = slice(np.nanmin(N_high_crop), np.nanmax(N_high_crop)))\n",
    "t_max_d1 = t_max.sel(E=slice(np.nanmin(E_high_crop), np.nanmax(E_high_crop)), N = slice(np.nanmin(N_high_crop), np.nanmax(N_high_crop)))\n",
    "t_min_d1 = t_min.sel(E=slice(np.nanmin(E_high_crop), np.nanmax(E_high_crop)), N = slice(np.nanmin(N_high_crop), np.nanmax(N_high_crop)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f39873",
   "metadata": {},
   "outputs": [],
   "source": [
    "LONS_D1 = E_axis[np.logical_and(E_axis >= E_high_crop[0], E_axis < E_high_crop[1])]\n",
    "LATS_D1 = N_axis[np.logical_and(N_axis >= N_high_crop[0], N_axis < N_high_crop[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee82ad1-2270-4792-9be2-d6b7ab93a6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(18,4))\n",
    "plot_map(axs[0], LONS_D1, LATS_D1, pr_d1.RhiresD.mean(dim='time').to_numpy().squeeze(), title=\"Daily precipitation\", cmap=mpl.cm.YlGnBu)\n",
    "#plot_map(axs[1], LONS_D1, LATS_D1, t_abs_d1.TabsD.mean(dim='time').to_numpy().squeeze(), title=\"Daily Temperature\", cmap=mpl.cm.RdBu_r)\n",
    "#plot_map(axs[2], LONS_D1, LATS_D1, t_max_d1.TmaxD.mean(dim='time').to_numpy().squeeze(), title=\"Daily Maximum temperature\", cmap=mpl.cm.RdBu_r)\n",
    "#plot_map(axs[3], LONS_D1, LATS_D1, t_min_d1.t_min.mean(dim='time').to_numpy().squeeze(), title=\"Daily Minimum temperature\", cmap=mpl.cm.RdBu_r)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9e275c8-038a-4012-bb9a-3783af9a0d7d",
   "metadata": {},
   "source": [
    "### Merge target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ca935c-65d7-43c2-93e1-e041134e5201",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#dy = xr.merge([pr_d1,t_abs_d1, t_max_d1, t_abs_d1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a428cdaa-9b57-46fd-913c-fbd126f7c5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now just prec\n",
    "dy = pr_d1.RhiresD\n",
    "# All CH-domain\n",
    "#dy = pr.RhiresD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a3adfa-990e-4396-9ebf-281f87923adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check NANs\n",
    "np.isnan(dy.to_numpy()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed68384-dd54-48bf-905e-d87765acb163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the predictors data\n",
    "# load predictors\n",
    "#l_paths = ['/precipitation/day_grid1/','/temperature/day_grid1/', '/U_wind/day_grid1/', '/V_wind/day_grid1/']\n",
    "#v_vars = ['tp','t', 'u', 'v']\n",
    "l_paths = ['/precipitation/day_grid1/','/temperature/day_grid1/']\n",
    "v_vars = ['tp','t']\n",
    "list_vars = load_data(v_vars, l_paths, G, PATH_low, DATE_START, DATE_END, LONS, LATS, LEVELS)\n",
    "datasets = list_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552a0bcb-b732-4cd3-a263-99595cfabe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.merge(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74da6d1a-78a9-4315-88a5-764db0794b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert lat axis if needed\n",
    "if ds.lat[0].values < ds.lat[1].values:\n",
    "    ds = ds.reindex(lat=list(reversed(ds.lat)))\n",
    "    \n",
    "# Get axes\n",
    "lats_x = ds.lat\n",
    "lons_x = ds.lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcf5c3c-a043-4d87-be6d-d651b5461cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "# only load a subset of the training data\n",
    "#ds_train = ds.sel(time=slice('1979', '2011')) \n",
    "#ds_valid = ds.sel(time=slice('2012', '2015')) \n",
    "#ds_test = ds.sel(time=slice('2016', '2021'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a48e64d-cc66-4273-bae5-f4e131c88c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dy_train = dy.sel(time=slice('1979', '2011'))\n",
    "#dy_valid = dy.sel(time=slice('2012', '2015'))\n",
    "#dy_test = dy.sel(time=slice('2016', '2021'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9046d4-1db6-44ba-947b-e82198696de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ds.sel(time=slice('1999', '2011')) \n",
    "ds_valid = ds.sel(time=slice('2012', '2015')) \n",
    "ds_test = ds.sel(time=slice('2016', '2021'))\n",
    "\n",
    "dy_train = dy.sel(time=slice('1999', '2011'))\n",
    "dy_valid = dy.sel(time=slice('2012', '2005'))\n",
    "dy_test = dy.sel(time=slice('2006', '2011'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe836e2-e95d-4540-a7b2-198459a1b51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {#'z': LEVELS,\n",
    "       'tp': None,\n",
    "       't': LEVELS}\n",
    "    #   'r': LEVELS,\n",
    "    #   'tcwv': None,\n",
    "     #  'u': LEVELS,\n",
    "     #  'v': LEVELS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18449efd-cc12-4b15-8d9f-0653447384f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dy_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12cde07-f83e-4115-ad6e-a3fa0a8d5729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data generator in pytorch # Adapted from the keras class\n",
    "class DataGenerator(Dataset):\n",
    "    def __init__(self, ds, label, var_dict, batch_size=32, load=True, mean=None, std=None, lead_time=None):\n",
    "        \"\"\"\n",
    "        Data generator for WeatherBench data.\n",
    "        Template from https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "        Args:\n",
    "            ds: Dataset containing all input variables\n",
    "            label: output\n",
    "            var_dict: Dictionary of the form {'var': level}. Use None for level if data is of single level\n",
    "            lead_time: Lead time in hours\n",
    "            batch_size: Batch size\n",
    "            shuffle: bool. If True, data is shuffled.\n",
    "            load: bool. If True, datadet is loaded into RAM.\n",
    "            mean: If None, compute mean from data.\n",
    "            std: If None, compute standard deviation from data.\n",
    "        \"\"\"\n",
    "        self.ds = ds\n",
    "        self.dy = label\n",
    "        self.var_dict = var_dict\n",
    "        self.batch_size = batch_size\n",
    "        self.lead_time = lead_time\n",
    "                \n",
    "        data = []\n",
    "        generic_level = xr.DataArray([1], coords={'level': [1]}, dims=['level'])\n",
    "        \n",
    "        for var, levels in var_dict.items():\n",
    "            if levels is None:\n",
    "                data.append(ds[var].expand_dims({'level': generic_level}, 1)) \n",
    "            else:\n",
    "                data.append(ds[var].sel(level=levels))\n",
    "\n",
    "        # Change level position \n",
    "        \n",
    "        self.data = xr.concat(data, 'level').transpose('time', 'lat', 'lon', 'level')\n",
    "        self.mean = self.data.mean(('time', 'lat', 'lon')).compute() if mean is None else mean\n",
    "        self.std = self.data.std('time').mean(('lat', 'lon')).compute() if std is None else std\n",
    "        \n",
    "        # Normalize\n",
    "        self.data = (self.data - self.mean) / self.std\n",
    "        \n",
    "        if self.lead_time is None: \n",
    "            self.n_samples = self.data.shape[0]\n",
    "        else:\n",
    "            self.n_samples = self.data.isel(time=slice(0, -lead_time)).shape[0]\n",
    "            self.init_time = self.data.isel(time=slice(None, -lead_time)).time\n",
    "            self.valid_time = self.data.isel(time=slice(lead_time, None)).time\n",
    "            \n",
    "        self.idxs = np.arange(self.n_samples)\n",
    "        \n",
    "\n",
    "        # For some weird reason calling .load() earlier messes up the mean and std computations\n",
    "        if load: print('Loading data into RAM'); self.data.load()\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idxs = self.idxs[idx]\n",
    "        \n",
    "        X = (torch.Tensor(self.data.isel(time=idxs).values))\n",
    "        \n",
    "        if self.lead_time is None:\n",
    "             y = (torch.Tensor(self.dy.isel(time=idxs).values))\n",
    "        else:\n",
    "             y = (torch.Tensor(self.dy.isel(time=idxs + self.lead_time).values))\n",
    "                \n",
    "        # In PyTorch we must transpose (C,H,W,B)\n",
    "        X = torch.Tensor(X).permute(2, 0, 1)\n",
    "        \n",
    "        if y.ndim > 2:\n",
    "            y = torch.Tensor(y).permute(2, 0, 1)\n",
    "        else:\n",
    "            # expand dimensions\n",
    "            y = torch.unsqueeze(y, dim=0)\n",
    "\n",
    "            \n",
    "        return X, y    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27464664-a9c9-431a-97ec-ff51a8961df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = DataGenerator(ds_train, dy_train, dic, batch_size=BATCH_SIZE, lead_time = None)\n",
    "loader_train = torch.utils.data.DataLoader(training_set, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd3bcd8-b6f7-4d1d-9fca-42d7ae7b4cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "valid_set = DataGenerator(ds_valid, dy_valid, dic, batch_size=BATCH_SIZE, mean=training_set.mean, std=training_set.std, lead_time = None)\n",
    "loader_val = torch.utils.data.DataLoader(valid_set, batch_size=32)\n",
    "# Test\n",
    "test_set = DataGenerator(ds_test, dy_test, dic, batch_size=BATCH_SIZE, mean=training_set.mean, std=training_set.std, lead_time = None)\n",
    "loader_test = torch.utils.data.DataLoader(test_set, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccdb8e7-0119-4d6f-a7c9-6266c9db0674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to make sure the range on the input and output images is correct, and they're the correct shape\n",
    "testx, testy = training_set.__getitem__(3)\n",
    "print(\"x shape: \", testx.shape)\n",
    "print(\"y shape: \", testy.shape)\n",
    "print(\"x min: \", torch.min(testx))\n",
    "print(\"x max: \", torch.max(testx))\n",
    "print(\"y min: \", torch.min(testy))\n",
    "print(\"y max: \",torch.max(testy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637a8ccc-b703-4ae8-af39-afd413646501",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(loader_train))\n",
    "x, y = data\n",
    "print('Shape of x:', x.shape)\n",
    "print('Shape of y:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98557915-f979-4949-b353-bdd1914882e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot input\n",
    "# Plotting the mean of the predictors\n",
    "n_figs = len(x[0,:,0,0])\n",
    "ncols = 3\n",
    "nrows = -(-n_figs // ncols)\n",
    "fig, axes = plt.subplots(figsize=(24, 3.3*nrows), ncols=ncols, nrows=nrows)\n",
    "for i in range(n_figs):\n",
    "    i_row = i // ncols\n",
    "    i_col = i % ncols\n",
    "    ax = axes[i_row, i_col]\n",
    "    vals = torch.mean(x[:,i,:,:],axis=0)\n",
    "    plot_map(ax, lons_x, lats_x, vals, title=f\"Average of feature {i+1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e41bdf0-6279-4b81-b669-5194d9edda29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the G and D\n",
    "# Adapted from https://github.com/mantariksh/231n_downscaling/blob/master/SRGAN.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2072e46f-3747-4add-8b90-375d9f7b8962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader, sampler, TensorDataset\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1758b33e-41ea-47a3-ae93-d0ab56fb35c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convenience \n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.size() # read in N, C, H, W\n",
    "        return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_channels, H, W):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential( \n",
    "            nn.Conv2d(in_channels=num_channels, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            Flatten(),  \n",
    "            nn.Linear(512 * int(np.ceil(H/16)) * int(np.ceil(W/16)), 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, 1),\n",
    "            nn.Sigmoid()\n",
    "            \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcea8d2e-0341-4752-a720-8689ec27b09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, num_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_channels=num_channels, out_channels=num_channels, kernel_size=3, stride=1, padding=0),\n",
    "            nn.InstanceNorm2d(num_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(num_channels, num_channels, kernel_size=3, stride=1, padding=0),\n",
    "            nn.InstanceNorm2d(num_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.layers(x)\n",
    "\n",
    "\n",
    "        \n",
    "class UpscaleBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, scale_factor):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.PixelShuffle(scale_factor),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, num_channels, input_size, num_res_blocks=16, scale_factor=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.input_size = input_size\n",
    "        self.initial_conv = nn.Sequential(\n",
    "            nn.ReflectionPad2d(4),\n",
    "            nn.Conv2d(num_channels, 64, kernel_size=9, stride=1, padding=0),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        \n",
    "        self.resBlocks = nn.ModuleList([ResidualBlock(64) for i in range(self.num_res_blocks)])\n",
    "\n",
    "        self.post_resid_conv = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(64, 64, 3, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "\n",
    "        self.upscale_blocks = nn.ModuleList()\n",
    "        for _ in range(int(math.log(scale_factor, 2))):\n",
    "            self.upscale_blocks.append(UpscaleBlock(64, 64 * 4, scale_factor=2))\n",
    "\n",
    "        self.conv_prelu = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(64, 64, 3, stride=1, padding=0),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "    \n",
    "        self.final_conv = nn.Sequential(\n",
    "            nn.ReflectionPad2d(4),\n",
    "            nn.Conv2d(64, 1, 9, stride=1, padding=0)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        initial_conv_out = self.initial_conv(x)\n",
    "                \n",
    "        res_block_out = self.resBlocks[0](initial_conv_out)\n",
    "        for i in range(1, self.num_res_blocks):\n",
    "            res_block_out = self.resBlocks[i](res_block_out)\n",
    "\n",
    "        post_resid_conv_out = self.post_resid_conv(res_block_out) + initial_conv_out\n",
    "\n",
    "        upscale_out = post_resid_conv_out\n",
    "        for block in self.upscale_blocks:\n",
    "            upscale_out = block(upscale_out)\n",
    "\n",
    "        conv_prelu_out = self.conv_prelu(upscale_out)\n",
    "        final_out = self.final_conv(conv_prelu_out)\n",
    "        # To get the final desired shape\n",
    "        final_out = F.interpolate(final_out, size=self.input_size, mode='bicubic', align_corners=True)\n",
    "\n",
    "        return final_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da98a192-f9e0-4a8b-82e2-5080770c9fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test the Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfed46fd-617c-479e-9611-acfa89b6f7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CHANNELS_IN = 6\n",
    "dtype = torch.float32 \n",
    "input_size=y.shape[2:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd69e4c-f581-4897-bff4-673a3d7c27b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cbc42123-41a4-4982-9d96-9eb474a69ddc",
   "metadata": {},
   "source": [
    "### Check the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e67009b-a7b6-4c1a-a422-a4866545e452",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = (training_set.__getitem__(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1d20d9-f2cf-48a0-8333-c2ec011b8de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.unsqueeze(0)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9173f7ec-28d0-4db4-9cd7-7a341e0cfb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Generator(NUM_CHANNELS_IN,input_size)\n",
    "model = model.to(device=device)\n",
    "x = x.to(device=device, dtype=dtype)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c66a69-f17b-4200-b114-85f085d5dd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d690128-f07a-4fd6-8531-0e40e9667b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a339bc-eb38-45af-a2b6-7370db8975e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(121)\n",
    "plt.imshow(x.cpu().detach().numpy()[0, 0, :, :])\n",
    "plt.title(\"Input low-res Precip\")\n",
    "plt.subplot(122)\n",
    "plt.imshow(output.cpu().detach().numpy()[0, 0, :, :])\n",
    "plt.title(\"Output Precip\")\n",
    "plt.figure()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed01be9b-b01e-41cc-b56a-5952e09cc251",
   "metadata": {},
   "source": [
    "### Check the discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d64f9c-c88e-46c8-9190-902026568f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = (training_set.__getitem__(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f626cf40-48fd-46ca-b9ec-d1c3412e6530",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da58e15-004d-44a7-909d-5743ffa08ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.unsqueeze(0)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2464dea-1526-4b69-8a1b-121b16072990",
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = y.shape[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f112def-788e-4a8a-8d9e-dd4864a765e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Test the discriminator\n",
    "def test_Discriminator():\n",
    "    x,y = (training_set.__getitem__(3))\n",
    "    y = y.unsqueeze(0)\n",
    "    print(\"y: \", y.shape)\n",
    "    model = Discriminator(num_channels=1, H=h,W=w)\n",
    "    output = model(y)\n",
    "    print(output.size())\n",
    "    print(output)\n",
    "#test_Discriminator()\n",
    "\n",
    "\n",
    "def test_withnan_Discriminator():\n",
    "    x, y = training_set.__getitem__(3)\n",
    "    y = y.unsqueeze(0)\n",
    "    print(\"y shape:\", y.shape)\n",
    "\n",
    "    # Replace NaN values with a valid value\n",
    "    y = torch.where(torch.isnan(y), torch.zeros_like(y), y)\n",
    "\n",
    "    model = Discriminator(num_channels=1, H=h, W=w)\n",
    "    output = model(y)\n",
    "    print(\"Output size:\", output.size())\n",
    "    print(output)\n",
    "\n",
    "test_withnan_Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e840c5bb-5267-4285-853d-62991a4b4bf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generator_withNan_loss(gen_img, true_img, logits_fake, weight_param=1e-3):\n",
    "    \"\"\"\n",
    "    Computes the generator loss described above.\n",
    "\n",
    "    Inputs:\n",
    "    - gen_img: (PyTorch tensor) shape N, C image generated by the Generator, so that we can calculate MSE\n",
    "    - true_img: (PyTorch tensor) the true, high res image, so that we can calculate the MSE\n",
    "    - logits_fake: PyTorch Tensor of shape (N,) giving scores for the fake data.\n",
    "    - weight_param: how much to weight the adversarial loss by when summing the losses. Default in Ledig paper is 1e-3\n",
    "    \n",
    "    Returns:\n",
    "    - loss: PyTorch Tensor containing the (scalar) loss for the generator.\n",
    "    \"\"\"\n",
    "    #\n",
    "    if torch.isnan(gen_img).any() or torch.isnan(true_img).any() or torch.isnan(logits_fake).any():\n",
    "        # Handle NaN values here\n",
    "        # Replace NaN values in gen_img and true_img with zeros\n",
    "        gen_img = torch.where(torch.isnan(gen_img), torch.zeros_like(gen_img), gen_img)\n",
    "        true_img = torch.where(torch.isnan(true_img), torch.zeros_like(true_img), true_img)\n",
    "\n",
    "    \n",
    "    # Content loss - MSE loss\n",
    "    content_loss_func = nn.MSELoss()\n",
    "    content_loss = content_loss_func(gen_img, true_img)\n",
    "        \n",
    "    N = logits_fake.shape[0]\n",
    "    desired_labels = torch.ones(N, 1).to(device=device, dtype=dtype)\n",
    "    BCE_Loss = nn.BCELoss()\n",
    "    adversarial_loss = BCE_Loss(logits_fake, desired_labels)\n",
    "    \n",
    "    total_loss = content_loss + weight_param * adversarial_loss\n",
    "    \n",
    "    return total_loss, content_loss, adversarial_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42260d7a-54bd-4dab-96d4-5748d8412470",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generator_withNaN_loss(gen_img, true_img, logits_fake, weight_param=1e-3):\n",
    "    \"\"\"\n",
    "    Computes the generator loss described above.\n",
    "\n",
    "    Inputs:\n",
    "    - gen_img: (PyTorch tensor) shape N, C image generated by the Generator, so that we can calculate MSE\n",
    "    - true_img: (PyTorch tensor) the true, high res image, so that we can calculate the MSE\n",
    "    - logits_fake: PyTorch Tensor of shape (N,) giving scores for the fake data.\n",
    "    - weight_param: how much to weight the adversarial loss by when summing the losses. Default in Ledig paper is 1e-3\n",
    "    \n",
    "    Returns:\n",
    "    - loss: PyTorch Tensor containing the (scalar) loss for the generator.\n",
    "    \"\"\"\n",
    "    # Handle NaN values in gen_img and true_img\n",
    "    gen_img = torch.where(torch.isnan(gen_img), torch.zeros_like(gen_img), gen_img)\n",
    "    true_img = torch.where(torch.isnan(true_img), torch.zeros_like(true_img), true_img)\n",
    "    \n",
    "    # Content loss - MSE loss\n",
    "    content_loss_func = nn.MSELoss()\n",
    "    content_loss = content_loss_func(gen_img, true_img)\n",
    "        \n",
    "    N = logits_fake.shape[0]\n",
    "    desired_labels = torch.ones(N, 1).to(device=device, dtype=dtype)\n",
    "    BCE_Loss = nn.BCELoss()\n",
    "    adversarial_loss = BCE_Loss(logits_fake, desired_labels)\n",
    "    \n",
    "    total_loss = content_loss + weight_param * adversarial_loss\n",
    "    \n",
    "    return total_loss, content_loss, adversarial_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a0a5c4-1d76-4580-bfa1-2ba919a286d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_with_Nan_loss(logits_real, logits_fake):\n",
    "    \"\"\"\n",
    "    Computes the discriminator loss described above.\n",
    "    \n",
    "    Inputs:\n",
    "    - logits_real: PyTorch Tensor of shape (N,) giving scores for the real data (real numbers). \n",
    "    - logits_fake: PyTorch Tensor of shape (N,) giving scores for the fake data (real numbers).\n",
    "    \n",
    "    Returns:\n",
    "    - loss: PyTorch Tensor containing the loss for the discriminator.\n",
    "    \"\"\"\n",
    "    # Handle NaN values\n",
    "    if torch.isnan(logits_real).any() or torch.isnan(logits_fake).any():\n",
    "        # Replace NaN values with zeros\n",
    "        logits_real = torch.where(torch.isnan(logits_real), torch.zeros_like(logits_real), logits_real)\n",
    "        logits_fake = torch.where(torch.isnan(logits_fake), torch.zeros_like(logits_fake), logits_fake)\n",
    "\n",
    "    N = logits_real.shape[0]\n",
    "    real_labels = torch.ones(N, 1).to(device=logits_real.device, dtype=logits_real.dtype)\n",
    "    fake_labels = torch.zeros(N, 1).to(device=logits_fake.device, dtype=logits_fake.dtype)\n",
    "    \n",
    "    BCE_Loss = nn.BCELoss()\n",
    "    L1 = BCE_Loss(logits_real, real_labels)\n",
    "    L2 = BCE_Loss(logits_fake, fake_labels)\n",
    "    \n",
    "    loss = L1 + L2\n",
    "    return loss, L1, L2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495f2223-2d61-4c4f-b1e0-8d89ccfd7340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def check_generator_with_nan_accuracy(loader, model):\n",
    "    model.eval()\n",
    "    count, rmse_precip_ypred, rmse_precip_x = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            model = model.to(device=device)\n",
    "            y = y.to(device=device, dtype=dtype)\n",
    "            \n",
    "            x_np = x.numpy()\n",
    "            x_min = np.amin(x_np, axis=(2,3))[:, :, np.newaxis, np.newaxis]\n",
    "            x_max = np.amax(x_np, axis=(2,3))[:, :, np.newaxis, np.newaxis]\n",
    "            is_nan = np.int((x_min == x_max).any())\n",
    "            eps = 1e-9\n",
    "            x_norm_np = (x_np - x_min) / ((x_max - x_min + is_nan*eps) / 2) - 1\n",
    "            x_norm_np[np.isnan(x_norm_np)] = 0  # Replace NaN values with zeros\n",
    "            \n",
    "            x_norm = torch.from_numpy(x_norm_np)\n",
    "            x_norm = x_norm.to(device=device, dtype=dtype)\n",
    "            x = x.to(device=device, dtype=dtype)\n",
    "            \n",
    "            y_predicted = model(x)\n",
    "            y_predicted[np.isnan(y_predicted)] = 0  # Replace NaN values with zeros\n",
    "            \n",
    "            rmse_precip_ypred += torch.sqrt(torch.mean((y_predicted[:,0,:,:]-y[:,0,:,:]).pow(2)))\n",
    "            rmse_precip_x += torch.sqrt(torch.mean((x_norm[:,0,:,:]-y[:,0,:,:]).pow(2)))\n",
    "            count += 1\n",
    "            \n",
    "        rmse_precip_ypred /= count\n",
    "        rmse_precip_x /= count\n",
    "        print('RMSEs: \\tInput precip: %.3f\\n\\tOutput precip: %.3f\\n\\t' % \n",
    "              (rmse_precip_x, rmse_precip_ypred))\n",
    "        \n",
    "        \n",
    "def check_discriminator_with_nan_accuracy(loader, D, G):\n",
    "    D = D.to(device=device)\n",
    "    G = G.to(device=device)\n",
    "    D.eval()\n",
    "    G.eval()\n",
    "    \n",
    "    count, avg_true_pred, avg_fake_pred = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)\n",
    "            y = y.to(device=device, dtype=dtype)\n",
    "            \n",
    "            true_pred = D(y)\n",
    "            true_pred[np.isnan(true_pred)] = 0  # Replace NaN values with zeros\n",
    "            avg_true_pred += true_pred.sum()\n",
    "            count += len(true_pred)\n",
    "            \n",
    "            fake_imgs = G(x)\n",
    "            fake_pred = D(fake_imgs)\n",
    "            fake_pred[np.isnan(fake_pred)] = 0  # Replace NaN values with zeros\n",
    "            avg_fake_pred += fake_pred.sum()\n",
    "            \n",
    "        avg_true_pred /= count\n",
    "        avg_fake_pred /= count\n",
    "        print(\"Average prediction score on real data: %f\" % (avg_true_pred))\n",
    "        print(\"Average prediction score on fake data: %f\" % (avg_fake_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc821d6-2a7f-40d1-8512-b93d7364425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper functions for plotting\n",
    "def plot_epoch(x, y_pred, y):\n",
    "    figsize = (9,4)\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(x[0,0,:,:].cpu().detach().numpy())\n",
    "    plt.title(\"Input Precip\")\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(y_pred[0,0,:,:].cpu().detach().numpy())\n",
    "    plt.title(\"Output Precip\")\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(y[0,0,:,:].cpu().detach().numpy())\n",
    "    plt.title(\"True Precip\")\n",
    "    \n",
    "    \n",
    "def plot_loss(G_content, G_advers, D_real_L, D_fake_L, weight_param):\n",
    "    \n",
    "    D_count = np.count_nonzero(D_real_L)\n",
    "    G_count = np.count_nonzero(G_content)\n",
    "    \n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(range(G_count), G_content[range(G_count)])\n",
    "    plt.plot(range(G_count), G_advers[range(G_count)])\n",
    "    plt.plot(range(G_count), G_content[range(G_count)] + weight_param*G_advers[range(G_count)])\n",
    "    plt.legend((\"Content\", \"Adversarial\", \"Total\"))\n",
    "    plt.title(\"Generator loss\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(range(D_count), D_real_L[range(D_count)])\n",
    "    plt.plot(range(D_count), D_fake_L[range(D_count)])\n",
    "    plt.plot(range(D_count), D_real_L[range(D_count)] + D_fake_L[range(D_count)])\n",
    "    plt.legend((\"Real Pic\", \"Fake Pic\", \"Total\"))\n",
    "    plt.title(\"Discriminator loss\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7b5324-d144-405a-8720-108af1a7db9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778c9af6-f138-4f04-9038-998b19fabb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = Discriminator(num_channels=1, H=h,W=w) \n",
    "G = Generator(NUM_CHANNELS_IN, input_size)\n",
    "\n",
    "lr = 0.0002\n",
    "# No checkpoints....\n",
    "# Define optimizer for discriminator\n",
    "D_solver = torch.optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "# Define optimizer for generator\n",
    "G_solver = torch.optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c48ecd-11a7-4dfb-ab0f-2f767f4dbffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=2\n",
    "G_iters=1\n",
    "dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00007085-a261-4a36-a664-99cd9385af57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the models to the correct device (GPU if GPU is available)\n",
    "D = D.to(device=device)\n",
    "G = G.to(device=device)\n",
    "    \n",
    "# Put models in training mode\n",
    "D.train()\n",
    "G.train()\n",
    "    \n",
    "print(\"Expected num iters: \", len(loader_train)*num_epochs)\n",
    "G_content = np.zeros(len(loader_train)*num_epochs*G_iters+1)\n",
    "G_advers = np.zeros(len(loader_train)*num_epochs*G_iters+1)\n",
    "D_real_L = np.zeros(len(loader_train)*num_epochs+1)\n",
    "D_fake_L = np.zeros(len(loader_train)*num_epochs+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d19042-34c7-45c5-9d64-9168a7992a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#high_res_imgs = y.to(device=device, dtype=dtype)\n",
    "#logits_real = D(high_res_imgs)\n",
    "\n",
    "#x.requires_grad_()\n",
    "#low_res_imgs = x.to(device=device, dtype=dtype)\n",
    "#fake_images = G(low_res_imgs)\n",
    "#logits_fake = D(fake_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b65f1d-2a84-42bf-8fe3-9af68eee852d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fake_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea10af9-ccdd-480e-9296-f8b1f29df472",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logits_fake.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ce0424-de72-4d91-a26d-dd5cf5ba4f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#d_total_error, D_real_L[iter_count], D_fake_L[iter_count] = discriminator_loss(logits_real, logits_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7495abff-e819-496f-a6b6-ce2d0a16a0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#discriminator_loss(logits_real, logits_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f350c36-4da1-40af-98bb-d543a3d46a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################TEST\n",
    "#iter_count = 0\n",
    "#G_iter_count = 0\n",
    "#show_every=40\n",
    "#tic = time()\n",
    "#for epoch in range(num_epochs):\n",
    "#    for x,y in loader_train:\n",
    "#        high_res_imgs = y.to(device=device, dtype=dtype)\n",
    "#        logits_real = D(high_res_imgs)\n",
    "\n",
    "#       x.requires_grad_()\n",
    "#        low_res_imgs = x.to(device=device, dtype=dtype)\n",
    "#        fake_images = G(low_res_imgs)\n",
    "#        logits_fake = D(fake_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565fddc3-dff7-47f3-b58b-78423e8f1d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#d_total_error, D_real_L[iter_count], D_fake_L[iter_count] = discriminator_with_Nan_loss(logits_real, logits_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bc60f5-51d2-42b7-b890-473664e26dae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iter_count = 0\n",
    "G_iter_count = 0\n",
    "show_every=40\n",
    "tic = time()\n",
    "for epoch in range(num_epochs):\n",
    "    for x,y in loader_train:\n",
    "        high_res_imgs = y.to(device=device, dtype=dtype)\n",
    "        logits_real = D(high_res_imgs)\n",
    "\n",
    "        x.requires_grad_()\n",
    "        low_res_imgs = x.to(device=device, dtype=dtype)\n",
    "        fake_images = G(low_res_imgs)\n",
    "        logits_fake = D(fake_images)\n",
    "    \n",
    "        # Update for the discriminator\n",
    "        #d_total_error, D_real_L[iter_count], D_fake_L[iter_count] = discriminator_with_Nan_loss(logits_real, logits_fake)\n",
    "        d_total_error, D_real_L[iter_count], D_fake_L[iter_count] = discriminator_loss(logits_real, logits_fake)\n",
    "        #print('d_total_error:', d_total_error)\n",
    "        #print('D_real_L[iter_count]:', D_real_L[iter_count])\n",
    "        #print('D_fake_L[iter_count]:', D_fake_L[iter_count])\n",
    "        D_solver.zero_grad()\n",
    "        d_total_error.backward()\n",
    "        D_solver.step()\n",
    "        \n",
    "        for i in range(G_iters):\n",
    "                # Update for the generator\n",
    "                fake_images = G(low_res_imgs)\n",
    "                logits_fake = D(fake_images)\n",
    "                gen_logits_fake = D(fake_images)\n",
    "                weight_param = 1e-1 # Weighting put on adversarial loss\n",
    "                g_error, G_content[G_iter_count], G_advers[G_iter_count] = generator_loss(fake_images, high_res_imgs, gen_logits_fake, weight_param=weight_param)\n",
    "                #g_error, G_content[G_iter_count], G_advers[G_iter_count] = generator_withNan_loss(fake_images, high_res_imgs, gen_logits_fake, weight_param=weight_param)\n",
    "                \n",
    "                G_solver.zero_grad()\n",
    "                g_error.backward()\n",
    "                G_solver.step()\n",
    "                G_iter_count += 1\n",
    "                \n",
    "        if (iter_count % show_every == 0):\n",
    "                toc = time()\n",
    "                print('Epoch: {}, Iter: {}, D: {:.4}, G: {:.4}, Time since last print (min): {:.4}'.format(epoch,iter_count,d_total_error.item(),g_error.item(), (toc-tic)/60 ))\n",
    "                tic = time()\n",
    "                plot_epoch(x, fake_images, y)\n",
    "                plot_loss(G_content, G_advers, D_real_L, D_fake_L, weight_param)\n",
    "                print()\n",
    "        iter_count += 1\n",
    "        \n",
    "        D = D.to(device=device)\n",
    "        G = G.to(device=device)\n",
    "        # Put models in training mode\n",
    "        D.train()\n",
    "        G.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c46cecd-cdb0-433d-8779-d914581314ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e00b46-5485-4c51-b277-49b87f101c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_pytorch",
   "language": "python",
   "name": "venv_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
